{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to LangMem\n",
    "\n",
    "This is an early preview of LangMem, a longterm memory service built by LangChain designed help you eaily build personalized user experiences with LLMs. We will walk through the basic functionality as an orientation to the service, including:\n",
    "\n",
    "1. Creating memory types\n",
    "2. Posting messages to the service to trigger memory formation\n",
    "3. Recalling the memories for use in your bot.\n",
    "\n",
    "My incorporating this in your chat bot, LangMem will asynchronously help it learn their preferences and interests to improve the quality of its responses and recommendations.\n",
    "\n",
    "Replicated from [this blog post](https://langchain-ai.github.io/long-term-memory/quick_start/#0-environment-setup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment Setup\n",
    "We've created a demo `quickstart` instance for you to try out in this notebook.\n",
    "\n",
    "While LangMem is still in private alpha, you'll need to be given access to a dedicated LangMem instance from a member LangChain team for more personal use beyond this demo. Reach out on Slack or at support@langchain.dev to receive your connection information.\n",
    "\n",
    "Then you'll want to install the sdk (we will also use openai in our example chat below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "# import anthropic\n",
    "from dotenv import load_dotenv\n",
    "from langmem import AsyncClient, Client\n",
    "import openai\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With the environment configured, you can create your client. We will connect to OpenAI and to Langmem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_client = openai.AsyncClient()\n",
    "# llm_client = anthropic.AsyncAnthropic()\n",
    "langmem_client = AsyncClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating custom memory types\n",
    "\n",
    "Memory types let you model your application domain so your bot can retain inforation in a format appropriate to your use case.\n",
    "\n",
    "LangMem supports both `user` level and `thread`-level memories.\n",
    "\n",
    "LangMem supports 3 primary types of user memories:\n",
    "\n",
    "1. Structured \"user state\" memory to infer and manage a pre-specified user profile\n",
    "2. Structured \"user append state\" memory to infer information salient to your application context and query semantically\n",
    "3. Unstructured user semantic memory\n",
    "\n",
    "The unstructured semantic memory is enabled by default in each deployment. We will see more of that below.\n",
    "\n",
    "Enabling structured memory functions is as easy as posting a schema to LangMem. LangMem then automatically manages these custom profiles whenever new messages are sent to the service.\n",
    "\n",
    "### User State\n",
    "This is a custom user profile. Instantiate by providing a JSON schema or pydantic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Person(BaseModel):\n",
    "    name: str = Field(default=None, description=\"The name of the family member.\")\n",
    "    relation: str = Field(\n",
    "        default=None, description=\"The relation of the family member to the user.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class UserProfile(BaseModel):\n",
    "    preferred_name: str = Field(default=None, description=\"The user's name.\")\n",
    "\n",
    "    summary: str = Field(\n",
    "        default=\"\",\n",
    "        description=\"A quick summary of how the user would describe themselves.\",\n",
    "    )\n",
    "    interests: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Short (two to three word) descriptions of areas of particular interest for the user. This can be a concept, activity, or idea. Favor broad interests over specific ones.\",\n",
    "    )\n",
    "    relationships: List[Person] = Field(\n",
    "        default_factory=Person,\n",
    "        description=\"A list of friends, family members, colleagues, and other relationships.\",\n",
    "    )\n",
    "    other_info: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "user_profile_memory = await langmem_client.create_memory_function(\n",
    "    UserProfile, target_type=\"user_state\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### User Append State\n",
    "As the name suggests, the `user_append_state` is an append-only state (meaning the profile is never overwritten) that lets you define schema(s) to represent individual memories which you can later query semantically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoreBelief(BaseModel):\n",
    "    belief: str = Field(\n",
    "        default=\"\",\n",
    "        description=\"The belief the user has about the world, themselves, or anything else.\",\n",
    "    )\n",
    "    why: str = Field(description=\"Why the user believes this.\")\n",
    "    context: str = Field(\n",
    "        description=\"The raw context from the conversation that leads you to conclude that the user believes this.\"\n",
    "    )\n",
    "\n",
    "\n",
    "belief_function = await langmem_client.create_memory_function(\n",
    "    CoreBelief, target_type=\"user_append_state\"\n",
    ")\n",
    "\n",
    "\n",
    "class FormativeEvent(BaseModel):\n",
    "    event: str = Field(\n",
    "        default=\"\",\n",
    "        description=\"The event that occurred. Must be important enough to be formative for the user.\",\n",
    "    )\n",
    "    impact: str = Field(default=\"\", description=\"How this event influenced the user.\")\n",
    "\n",
    "\n",
    "event_function = await langmem_client.create_memory_function(\n",
    "    FormativeEvent, target_type=\"user_append_state\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Semantic Memory\n",
    "\n",
    "There is also a triplet-based user_semantic_memory that is enabled by default. You can turn it off if you don't plan to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = langmem_client.list_memory_functions(target_type=\"user\")\n",
    "semantic_memory = None\n",
    "async for func in functions:\n",
    "    if func[\"type\"] == \"user_semantic_memory\":\n",
    "        semantic_memory = func\n",
    "\n",
    "\n",
    "# Uncomment to disable the unstructured memory\n",
    "# await langmem_client.update_memory_function(semantic_memory[\"id\"], status=\"disabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thread Summary\n",
    "\n",
    "LangMem also supports thread-level memories. We will create them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationSummary(BaseModel):\n",
    "    title: str = Field(description=\"Distinct for the conversation.\")\n",
    "    summary: str = Field(description=\"High level summary of the interactions.\")\n",
    "    topic: List[str] = Field(\n",
    "        description=\"Tags for topics discussed in this conversation.\"\n",
    "    )\n",
    "\n",
    "\n",
    "thread_summary_function = await langmem_client.create_memory_function(\n",
    "    ConversationSummary, target_type=\"thread_summary\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Starting a conversation\n",
    "Memories are formed whenever your chat bot posts messages to the service.\n",
    "\n",
    "Whenever a a user ID is provided in the message metadata, LangMem will automatically create a new user entry and start tracking memories for that user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "johnny_user_id = uuid.uuid4()\n",
    "jimmy_user_id = uuid.uuid4()\n",
    "jimmy_username = f\"jimmy-{uuid.uuid4().hex[:4]}\"\n",
    "johnny_username = f\"johnny-{uuid.uuid4().hex[:4]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example conversation between 1 or more users and an AIÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1980, there were several significant events that took place: \n",
      "1. The United States boycotted the Summer Olympics in Moscow in protest of the Soviet Union's invasion of Afghanistan.\n",
      "2. Mount St. Helens in the state of Washington erupted, causing significant damage and loss of life.\n",
      "3. Ronald Reagan was elected President of the United States, defeating incumbent Jimmy Carter.\n",
      "4. The Iran-Iraq War began, lasting for eight years and resulting in significant casualties.\n",
      "5. The introduction of the Rubik's Cube became a global sensation, captivating millions with its complex puzzle.\n",
      "\n",
      "These are just a few highlights of what was happening in 1980.\n"
     ]
    }
   ],
   "source": [
    "# Unique for a given converstaion\n",
    "thread_id = uuid.uuid4()\n",
    "\n",
    "\n",
    "async def completion(messages: list):\n",
    "    stripped_messages = [\n",
    "        {k: v for k, v in m.items() if k != \"metadata\"} for m in messages\n",
    "    ]\n",
    "    return await llm_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\", messages=stripped_messages\n",
    "    )\n",
    "\n",
    "async def claude_completion(messages: list):\n",
    "    # NOTE: this doesnt work. claude expects interleaving\n",
    "    # \"user\" and \"assistant\" messages.\n",
    "    system_message = messages[0]['content']\n",
    "    print(system_message)\n",
    "    stripped_messages = [\n",
    "        {k: v for k, v in m.items() if k not in ['metadata', 'name']} for m in messages[1:]\n",
    "    ]\n",
    "    print(stripped_messages)\n",
    "    return await llm_client.messages.create(\n",
    "        # model=\"gpt-3.5-turbo\", messages=stripped_messages\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        system=system_message,\n",
    "        max_tokens=2048,\n",
    "        messages=stripped_messages\n",
    "    )\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant\"},\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        # Names are optional but should be consistent with a given user id, if provided\n",
    "        \"name\": jimmy_username,\n",
    "        \"content\": \"Hey johnny have i ever told you about my older bro steve?\",\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(jimmy_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"no, you didn't, but I think he was friends with my younger sister sueann\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": johnny_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(johnny_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"yeah me and him used to play stickball down in the park before school started. I think it was in 1980\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": jimmy_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(jimmy_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"That was totally 1979! I remember because i was stuck at home all summer.\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": \"Jeanne\",\n",
    "        # If the user ID isn't provided, we treat this as a guest message and won't assign memories to the user\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"That was so long ago. I have gotten old and gained 200 pounds since then. I can't even remember who was president. @ai, who was the president in 1980?\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": johnny_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(johnny_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"The president of the United States in 1980 was Jimmy Carter.\",\n",
    "        \"role\": \"assistant\",\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Wow ya i forgot that. Stickleball really impacted my life. It's how i first met Jeanne! wonder how my life would have turned out if it hadn't happened that way.\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": jimmy_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(jimmy_user_id),\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"Yeah wow. That was a big year! @ai could you remind me what else was going on that year?\",\n",
    "        \"role\": \"user\",\n",
    "        \"name\": johnny_username,\n",
    "        \"metadata\": {\n",
    "            \"user_id\": str(johnny_user_id),\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "result = await completion(messages)\n",
    "\n",
    "messages.append(result.choices[0].message)\n",
    "print(result.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the messages, we can share them with LangMem.Â¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "await langmem_client.add_messages(thread_id=thread_id, messages=messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangMem will automatically process memories after some delay (~60 seconds), but we can eagerly process the memories as well\n",
    "await langmem_client.trigger_all_for_thread(thread_id=thread_id)\n",
    "# You could also trigger for a single user if you'd like\n",
    "# await langmem_client.trigger_all_for_user(user_id=jimmy_user_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Fetch messages\n",
    "\n",
    "You can fetch all the messages in a LangMem thread through that thread's GET endpoint. In this way, LangMem can act as a generic chat bot backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'You are a helpful AI assistant', 'role': 'system', 'name': None, 'metadata': {'user_id': None, 'timestamp': '2024-04-06T20:34:06.480831', 'id': '2f8852e9-2d6a-4b64-9d36-104008de9ec4'}}\n",
      "{'content': 'Hey johnny have i ever told you about my older bro steve?', 'role': 'user', 'name': 'jimmy-3ec3', 'metadata': {'user_id': '6156acd4-7dc7-4223-bfdd-164c85fcd841', 'timestamp': '2024-04-06T20:34:06.480857', 'id': '6880ccc5-5fe7-453b-9dce-b73c9d88d9b4'}}\n",
      "{'content': \"no, you didn't, but I think he was friends with my younger sister sueann\", 'role': 'user', 'name': 'johnny-318f', 'metadata': {'user_id': 'ea6ca039-456e-47c8-9322-b1a7e2a5c0bc', 'timestamp': '2024-04-06T20:34:06.480875', 'id': '255ffa74-042e-4171-8de5-897381c1e8e8'}}\n",
      "{'content': 'yeah me and him used to play stickball down in the park before school started. I think it was in 1980', 'role': 'user', 'name': 'jimmy-3ec3', 'metadata': {'user_id': '6156acd4-7dc7-4223-bfdd-164c85fcd841', 'timestamp': '2024-04-06T20:34:06.480904', 'id': 'cd1f0bca-24e3-4fa2-858e-cc63384d9b4d'}}\n",
      "{'content': 'That was totally 1979! I remember because i was stuck at home all summer.', 'role': 'user', 'name': None, 'metadata': {'user_id': None, 'timestamp': '2024-04-06T20:34:06.480923', 'id': 'dd3f4bd2-1585-4f56-8e1a-d3bf33008fea'}}\n",
      "{'content': \"That was so long ago. I have gotten old and gained 200 pounds since then. I can't even remember who was president. @ai, who was the president in 1980?\", 'role': 'user', 'name': 'johnny-318f', 'metadata': {'user_id': 'ea6ca039-456e-47c8-9322-b1a7e2a5c0bc', 'timestamp': '2024-04-06T20:34:06.480946', 'id': '8c9f45e9-b3f1-47d2-b24f-f7414cabddd4'}}\n",
      "{'content': 'The president of the United States in 1980 was Jimmy Carter.', 'role': 'assistant', 'name': None, 'metadata': {'user_id': None, 'timestamp': '2024-04-06T20:34:06.480964', 'id': '5acfa395-b4d0-4017-82c2-1ad13774fdee'}}\n",
      "{'content': \"Wow ya i forgot that. Stickleball really impacted my life. It's how i first met Jeanne! wonder how my life would have turned out if it hadn't happened that way.\", 'role': 'user', 'name': 'jimmy-3ec3', 'metadata': {'user_id': '6156acd4-7dc7-4223-bfdd-164c85fcd841', 'timestamp': '2024-04-06T20:34:06.480981', 'id': 'bfa9230f-a6cc-46aa-8779-9d99eb308912'}}\n",
      "{'content': 'Yeah wow. That was a big year! @ai could you remind me what else was going on that year?', 'role': 'user', 'name': 'johnny-318f', 'metadata': {'user_id': 'ea6ca039-456e-47c8-9322-b1a7e2a5c0bc', 'timestamp': '2024-04-06T20:34:06.480998', 'id': 'e922af3d-4b2b-4d38-8fec-813b011bbbb6'}}\n",
      "{'content': \"In 1980, there were several significant events that took place: \\n1. The United States boycotted the Summer Olympics in Moscow in protest of the Soviet Union's invasion of Afghanistan.\\n2. Mount St. Helens in the state of Washington erupted, causing significant damage and loss of life.\\n3. Ronald Reagan was elected President of the United States, defeating incumbent Jimmy Carter.\\n4. The Iran-Iraq War began, lasting for eight years and resulting in significant casualties.\\n5. The introduction of the Rubik's Cube became a global sensation, captivating millions with its complex puzzle.\\n\\nThese are just a few highlights of what was happening in 1980.\", 'role': 'assistant', 'name': None, 'metadata': {'user_id': None, 'timestamp': '2024-04-06T20:34:06.481016', 'id': '51f12282-54d5-45ff-9cb1-528211e615c5'}}\n"
     ]
    }
   ],
   "source": [
    "messages = langmem_client.list_messages(thread_id=thread_id)\n",
    "async for message in messages:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Query Memory\n",
    "\n",
    "You can also query the user memory, once it's updated. This may take a few moments - please be patient ðŸ˜Š\n",
    "\n",
    "To query the unstructured semantic memory, you can provide query text and the number of memories to return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'ac7f103f-9adc-4bc3-a21d-b13ca958eb9e',\n",
       "  'created_at': '2024-04-06T20:34:10.205039Z',\n",
       "  'last_accessed': '2024-04-06T20:34:10.205039Z',\n",
       "  'text': '',\n",
       "  'content': {'belief': 'I have an older brother named Steve.',\n",
       "   'why': 'The user mentioned having an older brother named Steve.',\n",
       "   'context': 'Hey johnny have i ever told you about my older bro steve?'},\n",
       "  'scores': {'recency': 1.0, 'importance': 0.5, 'relevance': 0.0}},\n",
       " {'id': '22d9d33d-5e25-4906-a62e-9af36f510ba7',\n",
       "  'created_at': '2024-04-06T20:34:10.071304Z',\n",
       "  'last_accessed': '2024-04-06T20:34:10.071304Z',\n",
       "  'text': '',\n",
       "  'content': {'event': 'Playing stickball in the park before school in 1980',\n",
       "   'impact': \"Significant impact on user's life, including meeting their partner Jeanne.\"},\n",
       "  'scores': {'recency': 0.0, 'importance': 0.5, 'relevance': 1.0}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait a few moments for the memories to process. If this is empty, you'll likely have to wait a bit longer\n",
    "mems = None\n",
    "while not mems:\n",
    "    mem_response = await langmem_client.query_user_memory(\n",
    "        user_id=jimmy_user_id, text=\"stickleball\", k=3\n",
    "    )\n",
    "    mems = mem_response[\"memories\"]\n",
    "mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': '6156acd4-7dc7-4223-bfdd-164c85fcd841',\n",
       " 'memories': [{'id': 'ac7f103f-9adc-4bc3-a21d-b13ca958eb9e',\n",
       "   'created_at': '2024-04-06T20:34:10.205039Z',\n",
       "   'last_accessed': '2024-04-06T20:34:10.205039Z',\n",
       "   'text': '',\n",
       "   'content': {'belief': 'I have an older brother named Steve.',\n",
       "    'why': 'The user mentioned having an older brother named Steve.',\n",
       "    'context': 'Hey johnny have i ever told you about my older bro steve?'},\n",
       "   'scores': {'recency': 1.0, 'importance': 0.5, 'relevance': 0.0}},\n",
       "  {'id': '22d9d33d-5e25-4906-a62e-9af36f510ba7',\n",
       "   'created_at': '2024-04-06T20:34:10.071304Z',\n",
       "   'last_accessed': '2024-04-06T20:34:10.071304Z',\n",
       "   'text': '',\n",
       "   'content': {'event': 'Playing stickball in the park before school in 1980',\n",
       "    'impact': \"Significant impact on user's life, including meeting their partner Jeanne.\"},\n",
       "   'scores': {'recency': 0.0, 'importance': 0.5, 'relevance': 1.0}}],\n",
       " 'state': None,\n",
       " 'thread_summaries': None}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In a similar way, you can include\n",
    "# different `user_append_state` memory results\n",
    "# in the ranked response\n",
    "mems = await langmem_client.query_user_memory(\n",
    "    user_id=jimmy_user_id,\n",
    "    text=\"stickleball\",\n",
    "    k=3,\n",
    "    memory_function_ids=[belief_function[\"id\"], event_function[\"id\"]],\n",
    ")\n",
    "mems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': None,\n",
       " 'interests': None,\n",
       " 'other_info': ['Played stickball in the park before school around 1979/1980.'],\n",
       " 'relationships': [{'name': 'Steve', 'relation': 'older brother'},\n",
       "  {'name': 'Jeanne', 'relation': 'significant other'}],\n",
       " 'preferred_name': 'Johnny'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For user state (profile) memories, you can make a faster and simple get request\n",
    "user_state = None\n",
    "while not user_state:\n",
    "    user_state = await langmem_client.get_user_memory(\n",
    "        user_id=jimmy_user_id, memory_function_id=user_profile_memory[\"id\"]\n",
    "    )\n",
    "user_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await langmem_client.list_thread_memory(thread_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use in a later conversations\n",
    "\n",
    "As you can see, we've extracted some useful information from the previous conversation. We imagine you would fetch these facts in later conversations to provide your bot with additional helpful context about the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, but I don't have access to personal information about specific individuals unless it has been shared with me in the course of our conversation. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "async def completion_with_memory(messages: list, user_id: uuid.UUID):\n",
    "    memories = await langmem_client.query_user_memory(\n",
    "        user_id=user_id, text=messages[-1][\"content\"], k=3\n",
    "    )\n",
    "    facts = \"\\n\".join([mem[\"text\"] for mem in memories[\"memories\"]])\n",
    "    system_prompt = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"You are a helpful assistant. You know the following facts about the user with which you are conversing.\\n\\n{facts}\",\n",
    "    }\n",
    "    return await completion([system_prompt] + messages)\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"name\": jimmy_username,\n",
    "        \"content\": \"Hi there! I'm curious what you remember. What's my brother's name?\",\n",
    "        \"metadata\": {\"user_id\": jimmy_user_id},\n",
    "    }\n",
    "]\n",
    "res = await completion_with_memory(messages, user_id=jimmy_user_id)\n",
    "print(res.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this walkthrough, you saved memories for two users to track their interests and other attributes. You did so simply by sending your chat messages to the LangMem service. You then automatically triggered updates to store long-term memories of three forms:\n",
    "\n",
    "User state \"profiles\", which follow your custom schema\n",
    "User append state, which store atomic memories following your custom schema and can be queried semantically.\n",
    "General-purpose knowledge as semantic triplets\n",
    "You also tracked thread-scoped summary memories to help you organize your conversational threads.\n",
    "\n",
    "Finally, let's clean up our work! This demo is a shared workspace :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AsyncClient' object has no attribute 'delete_memory_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# NOTE: this doesn't work because there's no delete_memory_function in async client\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mlangmem_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete_memory_function\u001b[49m(func[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AsyncClient' object has no attribute 'delete_memory_function'"
     ]
    }
   ],
   "source": [
    "## Cleanup\n",
    "\n",
    "functions = langmem_client.list_memory_functions()\n",
    "\n",
    "\n",
    "\n",
    "async for func in functions:\n",
    "    if func[\"type\"] == \"user_semantic_memory\":\n",
    "        continue\n",
    "    # NOTE: this doesn't work because there's no delete_memory_function in async client\n",
    "    await langmem_client.delete_memory_function(func[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
